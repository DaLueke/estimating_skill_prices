\documentclass{article}
\usepackage{amsmath} 											% formulas
\usepackage[bibstyle=authoryear,citestyle=authoryear]{biblatex} % citing and references style
\usepackage{booktabs}											% tables
\usepackage[left=3cm, top=2cm, right=2cm, bottom=2cm]{geometry} % spacing and edges
\usepackage{graphicx} 											% allows integration of images (.png files)

%\usepackage{array}
%\usepackage{makecell}
%\usepackage{arydshln}
%\renewcommand\theadalign{bc}
%\renewcommand\theadfont{\bfseries}
%\renewcommand\theadgape{\Gape[4pt]}
%\renewcommand\cellgape{\Gape[4pt]}

\newcommand\B{\rule[-1.5ex]{0pt}{0pt}}							% vertical spacing in tables

\DeclareMathOperator*{\argmin}{arg\,min}						% argmin operant

\bibliography{../references}

\title{Description of the Simulation Method}
\author{Daniel LÃ¼ke}

\begin{document}\maketitle

	This document describes the MC simulation that I implemented as part of my master's thesis. First, I will provide an overview of the theoretical framework that is the foundation of these simulations. In section 2 I sketch how an estimable version of this model results in an approximation error. After that, in section 3 I describe the wage simulation process, laying out the model parameters I choose. Finally, in section 4, I present the (biased) estimation results that are produced by the simulation and how these results change due to choice of the parameterization.\\


\section{Modeling Task Choice}
	At a starting point, let me briefly lay out the model that my simulations are based on.
	\begin{itemize}
		\item{In each period $t$, all agents $i$ maximize their utility $u_{i,t}$ by choosing the fraction of their work-time ($\lambda_{1,t}$ and $\lambda_{2,t}$) spent on each of two different tasks.}
		\item{Agents have no outside option from working on these tasks: $\lambda_{1,t} + \lambda_{2,t}=1$. Therefore, work-time is expressed in terms of $\lambda_{2,t} \equiv \lambda_t$ in the following.}
		\item{Agents dislike strong specialization and experience a penalty (dis-utility) from it.}
		\item{This penalty can be expressed as a function $p(\tau, \lambda, b_i, \phi)$ presented in equation \ref{eq:penalty} below.}
		
		% \begin{equation}\label{eq:penalty}
		% p(\tau, \lambda_i, b_i, \phi) = \left\{
		% 	 \begin{array}{ll}
		% 	 \tau (|b_i - \lambda_{i, t}|)^\phi, \: \text{s.t.} \: \phi \geq 1 &  \text{if $b_i \neq \lambda_{i, t}$}\\
		% 	 0, & \text{if $b_i = \lambda_{i, t}$}
		% 	 \end{array}
		% \right.
		% \end{equation}

		\begin{equation}\label{eq:penalty}
		p(\tau, \lambda_i, b_i, \phi) = 
			 \tau (|b_i - \lambda_{i, t}|)^\phi   % , \: \text{with} \: \phi \geq 1 \: \text{and} \: b_i, \lambda_{i, t} \in [0, 1], b_i \neq \lambda_{i, t}}}
		\end{equation}

		\item{Parameters of this penalty function:}

		\begin{itemize}	
			\item{$\tau$ - Penalty weight: Scales the penalty term. Impacts the extent agents deviate from their baseline task choice. High values reduce the impact of differences in potential wages on task choice. Low values allow for larger deviations from the baseline choices, increases the chance for corner solutions thereby. Assumed to be time-invariant and identical for all individuals.}
			\item{$b_i$ - Baseline task choice of a worker: This is the preferred task choice of a worker and marks the minimum of the penalty term. Assumed to be time-invariant and identical for all individuals.}
			\item{$\lambda_{i, t}$ - Endogenously determined task choice of a worker. Depends on each individual's potential wages in each period.}
			\item{$\phi$ - Penalty exponent: Scales penalty term. Since the base of this exponent by construction lies in $|b_i - \lambda_{i, t}| \in [0, 1]$, a larger exponent $\phi$ decreases penalties from deviations. $\phi$ needs to be lager than $1$ in order to ensure continuity of the first order condition (see section \ref{sec:FOC} below). $\phi$ is assumed to be time-invariant and identical for all individuals, too.} 
		\end{itemize}

		\item{Using the absolute value function in the penalty term comes at the costs of non-differentiability at $b_i = \lambda_{i, t}$. I will discuss this matter in section \ref{sec:FOC} below.}
		\item{Each agents utility $u_{i, t}$ is assumed to be her wage lessened by the penalty term.}

		\begin{equation}
			u_{i, t}(b_i, \lambda_{i, t}, \phi, \tau) = (1-\lambda_{i, t}) w_{1, i, t} + \lambda_{i, t} w_{2, i, t} - p(\tau, \lambda_{i, t}, b_i, \phi)
		\end{equation}
		\item{The agents' choice problem can be stated as follows.}

		\begin{equation} \label{eq:optimization_problem}
		\lambda^*_{i, t} \equiv \argmin_{\lambda_{i, t}} u_{i,t}(b_i, \lambda_{i, t}, \phi, \tau)
		\end{equation}

	\end{itemize}


\subsection{Finding optimal task choices}\label{sec:FOC}

		\begin{itemize}
			\item{In order to find the optimal task choice $\lambda^*_{i, t}$, following first order condition needs to be satisfied.}

			\begin{equation}
				\frac{\partial u_{i,t}(b_i, \lambda, \phi, \tau)}{\partial \lambda} \overset{!}{=} 0
			\end{equation}
			\item[]{where the partial derivative of the utility function w.r.t. $\lambda_{i,t}$ can be shown to be as follows.}

			% \begin{equation}\label{eq:FOC}
			% 	\frac{\partial u_{i,t}(b_i, \lambda, \phi, \tau)}{\partial \lambda}
			% 	\left\{
			% 	\begin{array}{ll}
			% 		\tilde{w} - \phi \tau \frac{(|b_i - \lambda|)^\phi}{b_i - \lambda}, \: & \: \text{if $b_i \neq \lambda$} \\
			% 		0, \: & \: \text{if $b_i = \lambda$}
			% 	\end{array}
			% 	\right.
			% \end{equation}

			\begin{equation}\label{eq:FOC}
				\frac{\partial u_{i,t}(b_i, \lambda_{i,t}, \phi, \tau)}{\partial \lambda_{i,t}} = \tilde{w} - \phi \tau \frac{(|b_i - \lambda_{i,t}|)^\phi}{b_i - \lambda_{i,t}}, \: \text{for $b_i \neq \lambda_{i,t}$}
			\end{equation}
			\item{Where $\tilde{w}$ denotes the relative potential wage, i.e., $\tilde{w} \equiv w_2 - w_1$.}
			\item{From equation \ref{eq:FOC} one can see that this first order condition is not defined at $b_i = \lambda_{i,t}$.}
			\item{Starting from equation \ref{eq:FOC} it can be shown that the optimal task choice results as follows. A derivation of this result can be found in appendix \ref{appen:lambda_opt}}

			\begin{equation} \label{eq:lmb_opt}
				\lambda^*_{i,t} = \left\{
				\begin{array}{ll}
					b_i - (\frac{- \tilde{w}}{\phi \tau})^{\frac{1}{\phi -1}}, \: & \: \text{if $b_i > \lambda_{i,t}$}\\
					b_i - (\frac{\tilde{w}}{\phi \tau})^{\frac{1}{\phi - 1}}, \: & \: \text{if $b_i < \lambda_{i,t}$}
				\end{array}
			\right.
			\end{equation}

			\item{Note that $\tilde{w}$ takes values smaller $0$, if and only if the potential wage in task $2$ is smaller then the potential wage in task $1$. This case is accompanied by a shift in task choices towards task $1$, which will result in $b_i \geq \lambda_i$. The exponent in equation (above), therefore, is by construction always over a strictly positive base and thereby defined for any real number of $\tilde{w}$. Analogously, it can be seen that $\tilde{w} > 0 \iff b_i < \lambda_{i,t}$.}

			% TODO: is this actually iff? Is this actually true for strict inequalities?


		\end{itemize}

\section{Estimation of Changes in Skill Prices}

	\begin{itemize}
		\item{Potential task specific wages (expressed in logs), $w_{i, j, t}$ are assumed to equal the sum of an individual's task specific (log) skills $s_{i,j}$ and task specific task prices $\pi_{j,t}$:}

		\begin{equation} \label{eq:pot_wage_equation}
		w_{i,j,t} = s_{i,j} + \pi_{j,t} \:, \:  j \in \{1, 2\}
		\end{equation}

		\item{In equation (\ref{eq:pot_wage_equation}), two assumptions are made implicitly. First, task specific individual skills do not depend on time. At this early stage, I abstract from skill accumulation and assume skills to be constant over time. Second, all individuals face the same task specific prices, which may change between time periods.}
		\item{The total wage any worker receives results as the sum of potential wages, each weighted by the time fraction the worker choses to spend on the particular task.}

		\begin{equation}
			w_{i,t} = (1- \lambda_{i,t}) w_{i, j=1, t} + \lambda_{i,t} w_{i, j=2, t}
		\end{equation}

		\item{In order to estimate changes in skill prices over discrete changes in time, I arrive at the following estimable equation for changes in skill prices. Arriving at this equation, I make use of an approximation. For a derivation, see appendix \ref{appen:wage_change_approx}}
		
		\begin{equation}
		\Delta w = \Delta \pi_{j=1} + \frac{\lambda_1 + \lambda_2}{2} \Delta \tilde{\pi}
		\end{equation}

		\item{Here, $\Delta$ indicates the change in a value from period $t=1$ to $t=2$. E.g., $\Delta w_i \equiv w_{i, t=2} - w_{i, t=1}$. The tilde indicates task-relative values, i.e., $\tilde{\pi_t} \equiv \pi_{j=2, t} - \pi_{j=1, t}$.}
		\item{This approximation results in an error causing estimation results to be biased. The true change in wages can be shown to be the following, where $\tilde{s_1}$ denotes the relative skills in period 1, i.e., $\tilde{s}_1 \equiv s_{j=2, t=1} - s_{j=1, t=1}$. A derivation of this result can be found in appendix \ref{appen:wage_change}}

		\begin{equation}
		\Delta w = \Delta \pi_{j=1} + \frac{\lambda_1 + \lambda_2}{2} \Delta \tilde{\pi} + \Delta \lambda (\frac{\tilde{\pi}_1 + \tilde{\pi}_2}{2} + \tilde{s}_1)
		\end{equation}

		\begin{itemize}
			\item{This approximation error term results in larger wage gains for individuals who:}

			\begin{itemize}
				\item[(1)]{Make larger adjustments towards task two between periods, i.e., have large $\Delta \lambda$.\footnote{It can be seen from equation(\ref{eq:lmb_opt}) that $\Delta \lambda = \lambda_{t=1} - \lambda_{t=0}$ increases in potential wages in task 2 and decreases in potential wages in task 1. Therefore, this additional gain in wages takes high values for individuals with high relative skills in task 2.}}
				\item[(2)]{Have high relative skill in task 2, i.e., have high $\tilde{s}_1$.}
			\end{itemize}

			\item[]{and vice versa.}
		\end{itemize}

		\item{Since both effects depend positively on the relative potential wage in task 2, the approximation error term results in a markup in wage gains for individuals with high relative skills in task 2.}
	\end{itemize}


\section[Simulation of Wage Changes]{Simulation of Wage Changes\footnote{A closer documentation of the code used for this simulation can be found in appendix \ref{appen:code_docu} of this document.} }
	\begin{itemize}
		\item{I simulate task specific skills in accordance to above stated assumptions: Skills do not change between periods. Thus, I draw skills from a multivariate normal distribution and keep them fixed for each agent. Means are assumed to be (5, 5) with a variance of 3 and a covariance of 0.3. Means as well as variance-covariance-matrix are model parameters that can be varied freely.}
		\item{Prices are assumed to change between periods. I assume that this change is deterministic and that growth rates are $(\Delta \pi_{j=1}, \Delta \pi_{j=2})=(0.05, 0.1)$. Starting values for these task specific skill prices are $(\pi_{j=1, t=0}, \pi_{j=2, t=0})=(5, 6)$.}
		\item{In each period, agents choose their work-time share $\lambda_{i,t}$ according to the optimization problem stated in equation (\ref{eq:lmb_opt}) above.}
		\item{Therefore, wage changes are exogenous in task specific skills and prices in this simulation.}
		\item{As part of a Monte Carlo Simulation, this simulation of wage changes is repeated $M$ times, where $M$ can be chosen freely.}
	\end{itemize}


\section{Results from changing the penalty term's specification}
	This section presents estimation results for a set of different specifications of the utility function, more precisely, the penalty term. This allows to get a better understanding of the approximation error as well as developing a strategy to limit its impact on the estimation results. For the sake of clarity, in each step I vary only two of the three penalty related model parameters: penalty weight $\tau$, the exponent of the penalty term, $p$, and the interval from which the baseline task choice is randomly drawn, $b_i$. This section is divided in three subsection. Each of which discusses the consequences of changing one of these parameters. \\
	For the first set of specifications, $b_i$ and $p$ are varied while $\tau$ is kept constant at a value of 30. The second set of specifications varies $b_i$ and $\tau$ while $p$ is held constant at a value of 2. \\ %TODO: Set \tau = 30 or 35 in both specifications -> see that results are the same!
	In addition to the mean estimation error and its standard deviation that are provided in tables (\ref{tab:est_rslt_differences_pp}) and (\ref{tab:est_rslt_differences_pw}), respectively, appendix \ref{appen:dist_mee} provides histograms of the distributions thereof.
	\subsection{Changing the range of baseline choices}
		\begin{itemize}
			\item{Agent's are assumed to have exogenous preferences for a specific split in work-time. This split is drawn from a uniform distribution over some interval.}
			\item{The columns of table (\ref{tab:est_rslt_differences_pp}) give the tuples of lower and upper border of this interval. For example, column name $(0.4, 0.6)$ can be interpreted as follows. For each agent $i$, the individual baseline task split $b_i$ is uniformly distributed in $b_i \sim U(0.4, 0.6)$}.
			\item{Allowing for differences in $b_i$ shifts the optimally chosen $\lambda$ for each agent towards her $b_i$.}
			\item{Differences in $b_i$ do not impact $\Delta \lambda$ (the factor of the approximation error term). This can be seen from \ref{eq:lmb_opt}: for a fixed agent $i$, changes in $\lambda$ between periods do not depend on $b_i$.}
			\item{While the difference in $\lambda$ between periods is not affected, the level of $\lambda$ is. Thus, introducing differences in $b_i$ leads to changes in $\frac{\lambda_1+\lambda_2}{2}$. This lowers the correlation between $\Delta \lambda$ and $\frac{\lambda_1+\lambda_2}{2}$ and thereby leads to lower biases in estimators.}
		\end{itemize}
	\subsection{Changing the exponent of the penalty term}
		\begin{itemize}
			\item{By definition of the penalty term $|b_i - \lambda| < 1$. Larger exponents of the penalty term $p$, thus, result in lower penalties from deviations.}
			\item{Setting $p$ to small values, i.e., $ p \in (1, 2)$ results in larger penalties from deviation from baseline split $b_i$. Hence, for such a specification agents will adjust their task choice relatively little. Therefore, smaller values for $\Delta \lambda$ are to be expected. This decreases the approximation error term.}
			\item{As seen from \ref{tab:est_rslt_differences_pp}, lowering the exponent of the penalty term results in overall smaller deviations of estimated price changes from true price changes.}
			\item{The case of $b_i = 0.5 \forall i \in N$ is an exception from this result as a heavy penalty for deviations paired with the same baseline decision for all agents results in $\lambda^* = 0.5$ for all agents. In this case, the regressor ($\frac{\lambda_1+\lambda_2}{2}$) takes constant value and is perfectly collinear with the constant.}
		\end{itemize}

		\begin{table}
		\begin{tabular}{lllll}
		\toprule
		Power of penalty 	& \multicolumn{4}{c}{Interval of baseline choice $(\underbar{b}, \bar{b})$}						\\
		term $p$			&						&						&						&						\\				
		\cmidrule{2-5}
						  	&  (0.5, 0.5)			&  (0.4, 0.6)			&  (0.3, 0.7)			&  (0.2, 0.8)			\\
		\midrule										
		1.25				&  -0.1128, 0.2255		&  -0.0, 0.0001			&  0.0, 0.0				&  0.0, 0.0				\\ \B
					      	&  (0.0777, 0.1555)		&  (0.0001, 0.0001)		&  (0.0, 0.0)			&  (0.0, 0.0)			\\
		1.5					&  -0.0505, 0.1009		&  -0.0002, 0.0009		&  0.0002, -0.0			&  0.0001, 0.0003		\\ \B
						  	&  (0.0022, 0.0043)		&  (0.0004, 0.0007)		&  (0.0002, 0.0004)		&  (0.0002, 0.0005)		\\
		2.0					&  -0.0248, 0.0497		&  -0.0071, 0.0154		&  -0.0018, 0.005		&  -0.0002, 0.0023		\\ \B
						  	&  (0.0002, 0.0003)		&  (0.001, 0.0018)		&  (0.0004, 0.0007)		&  (0.0006, 0.0012)		\\
		2.5					&  -0.0167, 0.0333		&  -0.0115, 0.0236		&  -0.0055, 0.0125		&  -0.0024, 0.007		\\ \B
						 	&  (0.0001, 0.0002)		&  (0.0007, 0.0009)		&  (0.0008, 0.0012)		&  (0.0006, 0.0011)		\\
		3.0					&  -0.0125, 0.025		&  -0.0105, 0.0214		&  -0.0069, 0.015		&  -0.0034, 0.009		\\ \B
						  	&  (0.0001, 0.0001)		&  (0.0004, 0.0007)		&  (0.0006, 0.0009)		&  (0.0004, 0.0007)		\\
		\bottomrule

		\end{tabular}
		\caption{Deviation of the estimation results from the respective true price changes. The first value in each cell relates to the estimation error of the price change in task $j=2$. The Second value to the estimation error of the relative price change between tasks $j=2$ and $j=1$: $(\Delta \hat{\pi}_{j=2} - \Delta \pi_{j=2},  \Delta \hat{\tilde{\pi}} - \Delta \tilde{\pi})$. Calculated for different specifications of the penalty term with respect to baseline decisions and the exponent of the penalty term. Provides the mean estimation error for $M = 100$ Monte Carlo simulations. Standard deviations are provided in brackets. Number of observations is $N = 100$, penalty weight $\tau$ is fixed at 30.}
		\label{tab:est_rslt_differences_pp}
		\end{table}

	\subsection{Changing the weight of the penalty term}
		\begin{itemize}
			\item{The penalty weight is a multiplier of the penalty term. Higher values of $\tau$, therefore, result in higher dis-utility from deviation from $b_i$ for all agents and, finally, in smaller adjustments to reflect changes in task prices.}
			\item{A very large value for $\tau$ will result in constant task choices $\lambda_i$ between periods. This eliminates the approximation error term. As presented in table (\ref{tab:est_rslt_differences_pw}), the mean estimation error generally decreases in $\tau$. Again, an exception is the special case of $b_i = 0.5 \forall i \in N$ where the argument remains the same as before: constant $b_i$ paired with high penalties results in perfect collinearity of the regressor and the constant.}
		\end{itemize}

		\begin{table}
		\begin{tabular}{lllll}
		\toprule
		Weight of the 		& \multicolumn{4}{c}{Interval of baseline choice $(\underbar{b}, \bar{b})$}						\\
		penalty term $\tau$	&						&						&						&						\\				
		\cmidrule{2-5}
						  	&  (0.5, 0.5)			&  (0.4, 0.6)			&  (0.3, 0.7)			&  (0.2, 0.8)			\\
		\midrule										
		5					&  -0.0199, 0.0401		&  -0.0186, 0.0378		&  -0.0131, 0.0279		&  -0.0082, 0.0193		\\ \B
					      	&  (0.0015, 0.0025)		&  (0.0021, 0.0037)		&  (0.0015, 0.0033)		&  (0.002, 0.0029)		\\
		20					&  -0.0251, 0.0502		&  -0.0118, 0.0253		&  -0.0044, 0.0107		&  -0.0013, 0.0049		\\ \B
						  	&  (0.0001, 0.0003)		&  (0.0009, 0.0017)		&  (0.0007, 0.0015)		&  (0.0007, 0.0014)		\\
		35					&  -0.025, 0.05			&  -0.0056, 0.0124		&  -0.0012, 0.0039		&  0.0001, 0.0014		\\ \B
						  	&  (0.0002, 0.0004)		&  (0.0011, 0.002)		&  (0.0006, 0.0011)		&  (0.0004, 0.0009)		\\
		50					&  -0.025, 0.0501		&  -0.003, 0.0069		&  -0.0005, 0.002		&  0.0001, 0.0008		\\ \B
						 	&  (0.0001, 0.0002)		&  (0.0005, 0.0011)		&  (0.0006, 0.0011)		&  (0.0004, 0.0007)		\\
		\bottomrule

		\end{tabular}
		\caption{Deviation of the estimation results from the respective true price changes. The first value in each cell relates to the estimation error of the price change in task $j=2$. The Second value to the estimation error of the relative price change between tasks $j=2$ and $j=1$: $(\Delta \hat{\pi}_{j=2} - \Delta \pi_{j=2},  \Delta \hat{\tilde{\pi}} - \Delta \tilde{\pi})$. Calculated for different specifications of the penalty term with respect to baseline decisions and the weight of the penalty term. Provides the mean estimation error for $M = 100$ Monte Carlo simulations. Standard deviations are provided in brackets. Number of observations is $N = 100$, the exponent of the penalty term is fixed at 2.}
		\label{tab:est_rslt_differences_pw}
		\end{table}


\newpage


\begin{appendix}


\section{Code Documentation --- outdated ---} \label{appen:code_docu} %TODO Rework this chapter: Wording, add other modules, rework referencing!
	\begin{itemize}
		\item{Drawing task specific skills, file "mc\_skills.py"}
		\begin{itemize}
			\item{By the assumption stated in previous section, there is no change in skills between time periods. Therefore, skills are drawn once and kept fixed for each agent.}
			\item{I draw each agent's skills from a multivariate random normal distribution with means $(5, 5)$ and a variance of 3. For now, skills can be defined to correlate by adjusting the variance-covariance matrix accordingly.}
		\end{itemize}
		\item{Drawing task specific skill prices, file "mc\_prices.py"}
		\begin{itemize}
			\item{I assume prices to change exogenously between periods. Changes are drawn from a uniform distribution over the interval $[-0.2, 0.2]$.}
			\item{Starting values for task specific skill prices are $(\pi_{j=1, t=0}, \pi_{j=2, t=0})=(5, 6)$.}
		\end{itemize}
		\item{Simulating optimal task choice, file "mc\_optimal\_wage\_choice.py"}
		\begin{itemize}
			\item{Using the previously generated task specific skills and task specific skill prices, I calculate each agent's utility according to the utility function defined in equation (\ref{eq:optimization_problem}).}
			\item{For this, each agent's baseline decision of her work-time split, $b_i$, is randomly drawn here. It is assumed to be uniform distributed within some borders that can be passed through the "p\_locus" argument.}
			\item{Both, the penalty weight $\tau$, as well as the exponent of the penalty term $p$ can be passed as arguments to this function.}
			\item{Provided these arguments of the penalty term as well as potential task specific wages for each period, I calculate the utility-maximizing work-time share $\lambda$ for each period.}
			\item{Finally, this file returns a table that provides the optimally chosen $\lambda$, resulting wage and resulting utility for each agent $i$ (rows) and each period $t$ (columns).}
		\end{itemize}
		\item{Calculating estimation results, file "mc\_estimation.py"}
		\begin{itemize}
			\item{In this last step I calculate the estimation results for set of combinations of boundaries for baseline work-time splits $b_i$ and the penalty term's exponent $p$.}
			\item{The results of this exercise can be seen in \ref{tab:est_rslt_differences_pp} where I calculated the difference between the estimated parameters and their true values.}
		\end{itemize}
	\end{itemize}


\section{Distribution of mean estimation errors} \label{appen:dist_mee}
\begin{figure}[ht]
\centering
	\includegraphics[angle=-90, scale=0.4]{C:/Users/danie/Documents/Master_Bonn/5_Semester/Thesis/social_skill_prices/MC_simulation/FIG/rslt_difference_dict_pp.png}
	\caption{Distribution of mean estimation errors for M=50 Monte Carlo simulations. \textbf{Power of the penalty term} is fixed for each row, interval for baseline task choices is fixed for each column.}
\end{figure}

\begin{figure}[ht]
	\includegraphics[angle=-90, scale=0.4]{C:/Users/danie/Documents/Master_Bonn/5_Semester/Thesis/social_skill_prices/MC_simulation/FIG/rslt_difference_dict_pw.png}
	\caption{Distribution of mean estimation errors for M=50 Monte Carlo simulations. \textbf{Weight of the penalty term} is fixed for each row, interval for baseline task choices is fixed for each column.}
\end{figure}


\section{Derivation of the optimal task choice} \label{appen:lambda_opt}

We start from the first order condition of the utility maximization:

\begin{equation}
	\frac{\partial u_{i, t}(b_i, \lambda, \phi, \tau, \tilde{w})}{\partial \lambda} = \tilde{w} - \phi \tau \frac{(|b_i - \lambda|)^\phi}{b_i - \lambda} \overset{!}{=} 0
\end{equation}

Defining $x \equiv b_i - \lambda$, this can be restated as follows.

\begin{alignat*}{3}
	{}					&& \tilde{w} - \phi \tau \frac{(|x|)^\phi}{x} 	&\overset{!}{=} 0 \\
	\Leftrightarrow 	&& \frac{(|x|)^\phi}{x} 						&= - \frac{\tilde{w}}{\phi \tau} \label{eq:foc_restated} \\
\end{alignat*}

The absolute value function for any real number $x$ is defined as follows.

\begin{equation}
	|x| = \left\{
		\begin{array}{ll}
			x, \: & \: \text{if $x \geq 0$} \\
			-x, \: & \: \text{if $x < 0$} \\
		\end{array}
	\right.
\end{equation}

Using this definition, two cases can be distinguished when solving for the optimal task choice $\lambda^*$. In section \ref{sec:FOC}, the domain of this problem is defined to exclude the case of $x=0$ (i.e., $b_i = \lambda_{i,t}$).\\
Case (1): $x > 0$.

\begin{alignat*}{3}
	{}				&& \frac{x^\phi}{x} 	&= - \frac{\tilde{w}}{\phi \tau} \\
	\Leftrightarrow && x					&= (- \frac{\tilde{w}}{\phi \tau})^{\frac{1}{\phi-1}}, \text{with $x = b_i - \lambda$} \\
	\Leftrightarrow && \lambda^* 			&= b_i - (- \frac{\tilde{w}}{\phi \tau})^{\frac{1}{\phi-1}}
\end{alignat*}

Case (2): $x < 0$.

\begin{alignat*}{3}
	{}				&& \frac{(-x)^\phi}{x} 					&= - \frac{\tilde{w}}{\phi \tau} \\
	\Leftrightarrow && \frac{-1}{-1} \frac{(-x)^\phi}{x} 	&=  - \frac{\tilde{w}}{\phi \tau} \\
	\Leftrightarrow && \frac{(-x)^\phi}{-x} 				&= \frac{\tilde{w}}{\phi \tau} \\
	\Leftrightarrow && x 									&= (\frac{\tilde{w}}{\phi \tau})^\frac{1}{\phi-1}, \text{with $x = b_i - \lambda$} \\
	\Leftrightarrow && \lambda^*							&= b_i - (\frac{\tilde{w}}{\phi \tau})^\frac{1}{\phi-1} \\
\end{alignat*}

\section{Calculating inter-temporal wage change}

\subsection{Approximation of the inter-temporal wage change} \label{appen:wage_change_approx}

\subsection{Determining the exact wage change} \label{appen:wage_change}



\end{appendix}




%
%		\item{For the penalty term, several different specifications are currently implemented:}
%		\begin{itemize}
%		\item{In the current version, penalty weight $\tau$ is set to 30. A value that is small enough to allow agents to deviate from their baseline split of work-time and large enough to prevent corner solutions.}
%		\item{Penalty power $p$ takes following range of values: }
%		\end{itemize}

\printbibliography
\end{document}